---
layout: post
title: AutoEncoder
tags: [AE, ML]
color: rgb(250, 50, 50)
feature-img: "assets/img/feature-img/ae1.png"
thumbnail: "assets/img/feature-img/ae1.png"
excerpt_separator: <!--more-->
---

---
## Methodology

NNs can extract features automatically once models are trained, allowing online monitoring using stochastic signals.
Auto-encoder, as one type of NNs, has caught more attention with its ability to reconstruct the data while reducing the
noise. Meanwhile, the bottleneck layer in the middle part of the network with a more compressed dimension gives
the opportunity to construct surrogate models with much fewer parameters. Based on this special feature, researchers
have developed damage detectors by comparing the latent variables of unknown signals to the ones of baseline data.
Some variations of auto-encoder such as CAE and LSTM-AE have also been developed and then used in the SHM
realm. The additional layers such as convolutional and LSTM layers make the models suitable to handle time series
data. Yet only deterministic results can be obtained, impeding the models to generate probabilistic results which could
be critical in statistically estimating structural status. Hence, variational auto-encoder (VAE) is adopted herein.
The mean and variation estimation of the latent variables in the bottleneck layer allow this type of models to provide
predictions along with the confidence intervals.

One VAE and two feedforward neural networks (FFNN) have been trained and combined into the framework as
shown in Figure 1. VAE is trained to minimize the distance between the output and original data. In this way, it can
be guaranteed that the latent vectors will contain features of the majority information of the tensors which can be
non-trivial to capture using conventional methods. The two FFNNs aim to build connections between latent vectors and
state vectors in opposite directions. After the training process, two different functions can be achieved by the trained
networks. First, given a new collected time series, it will be projected onto the latent space through the encoder and then
mapped to the corresponding state vector using the first FFNN. Second, given a certain state, the second trained FFNN
will map it to the corresponding latent vector which can then be stretched out to the original data shape.

## Variational Autoencoder (VAE)

VAE is a special type of auto-encoder. Auto-encoder, as a neural network, is commonly applied for data reconstruction.
This type of network can extract the hidden features in a different domain with much smaller dimension comparing to
the original data, therefore a compressed data representation can be achieved during the unsupervised training process.
The structure of an auto-encoder typically consists of an encoder and a decoder which can compress and extend data
dimension, respectively. By passing through the encoder, the data will be projected onto a latent vector z. Then the decoder will project the vector back to a tensor with the original shape.

## Framework
{% include aligner.html images="feature-img/feature-img/ae1.png" %}


## Latent Space

{% include aligner.html images="feature-img/feature-img/ae2.png, feature-img/feature-img/ae3.png" column=2 %}
{% include aligner.html images="feature-img/feature-img/ae4.png, feature-img/feature-img/ae5.png" column=2 %}
